<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Age and Gender Recognition Using CNN</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
    <link rel="stylesheet" href="css/css.css">
  </head>
  <body>
    <section class="page-header"> 
		
	<h1 class="project-name">Age Recognition Using Convolutional Neural Networks</h1>
	<h2 class="project-tagline">HMC CS152 Final Project by Olivia Watkins and Jasmine Zhu</h2>
    </section>

    <section class="main-content">
	
	<h1>
	<a id="user-content-header-1" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Problem</h1>

	The problem we are attempting to solve is the challenge of estimating a person's age from their face.  Rather than following the classic approach of treating this as a regression problem, we will approach 
	this problem using convolutional neural networks which consists of image classification and softmax. 
	
	<hr>


	<h1>
	<a id="user-content-header-1" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setup</h1>
	<h3>
	<a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset</h3>
	<p> We used the IMDB-WIKI data set, which contains 500k+ images of with age and gender labels scraped from the IMDB website and and Wikipedia. 
	It is the largest labeled age dataset available online. The data set is unblanced, containing a concentrated distribution of people ages 20 to 40. 
	

	<h3>
	<a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pre-Processing</h3>


	All pictures in this dataset are raw pictures scraped from websites and required pre-processing. To pre-process the images, we used the 
	<strong> dlib </strong> library to align the faces. This library returns the four coordinates of the box contianing the face. We chose to keep a 40% margin around the cropped faces because, according to a paper which also tried to estimeate the ages of individuals in this data set, the additional margin preserves more context information and improves the result. After that, we resized the faces to the same size (224x224) so they could be fed into our convolutional neural network. 

	Due to hardware limitations, we did not have the time or processing power to run our network on the entire data set. Instead, we only used the WIKI datasetm which contains 62,359 pictures. </p>


	<h3>
	<a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hardware</h3>
	
	<p> We implemented our model using the Keras library, with a TensorFlow backend. We've run our code on two GPUS - the Bridges supercomputer at the Pittsburgh Supercomputing Center (PSC) and an Amazon EC2 g2.2x large GPU using community image vict0sch, with 10GB memory. </p>
	
	<hr>






	<h1>
	<a id="user-content-header-1" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model</h1>
	
	<h3>
	<a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>I: VGG16</h3>
	
	<p> 
	<b><i>Motivation:</i></b>
	We first began by trying to implement the VGG16 architecture, an architecture commonly used for image classification problems. As shown in Figure 1, the VGG16 architecture contains 12 convolutional layers with ReLu activation functions interspersed with four max pooling layers, followed by several fully connected layers.
	</p>

	<img src="pics/vgg16Architecture.png"/>
	<br/>Figure 1: VGG16 Architecture<br/>

	<p>
	<b><i>Implementation:</i></b>
	We found a Keras implementation of the VGG16 architecture designed to be used with ImageNet.  The last layer of the model originally contained a softmax function which outputted a length-1000 array, the value of which represented the probability that the input belonged to each of the 1000 object classes. We modified the code to output a length-100 array, each index of which corresponds to one age.  We began with weights trained on ImageNet, then modified these weights as we trained on our own data set.
	</p>

	<p>
	<b><i>Result:</i></b>
	The model did not train very well.  Because the model was so large, training took a long time, so to train in a reasonable timeframe we were forced to only use around 2000 images from our dataset.  Even so, after several epochs, the training accuracy still remained very low and was not increasing. 
	</p>


	<div style="align:right">
		<img src="pics/fineTuningModel.png" class="right-pic"/>
		<br/>Figure 2: Top-level model we implemented on top of VGG16.<br/>
		<br/>
		<img src="pics/fineTuning.png" class="right-pic"/>
		<br/>Figure 3: This diagram shows our planned fine-tuning strategy. The blue blocks (part of the VGG16 architecture) would remain frozen while we trained our top-level model.<br/>
		<br/>
		<img src="pics/new Model.png" class="right-pic-big"/>
		<br/>Figure 4: New Convolutional Model<br/>
	</div>

	<h3>
	<a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fine-Tuning VGG16</h3>

	<p>
	<b><i>Motivation:</i></b> Properly training the full VGG16 architecture on our extensive data set would take too long on our hardware.  Fine-tuning a pretrained model on a new data set is a way to take advantage of the training done by others.
	</p>

	
	

	
	<p> 
	<b><i>Implementation:</i></b> Inspired by an implementation which fine-tuned a VGG16 model trained on ImageNet to differentiate dogs from cats, we created a top-level model containing two convolutional layers (the full model is shown in Figure 2).  We planned to first train our top-level model on our data set, then to refine the weights of the final convolutional block of the VGG16 model, and finally, to train the data all together (VGG16 model + our top-level model).  We were unable to find Keras model pretrained for any age estimation problem, so we instead tried to use a model pre-trained on ImageNet (which classifies objects, including people, into one of 1000 categories).  This process is shown in Fig 3. 
	</p>

	
	<p> 
	<b><i>Result:</i></b> We never got the chance to try the entire model together.  The fine-tuning approach involves first training the top-level model fairly well on the data set while the pre-trained model weights are frozen, then fine-tuning the pre-trained weights to improve accuracy. However, we were unable to accomplish the first step. Our accuracy began extremely low. The training accuracy improved marginally, but the validation accuracy showed little or no increase.
	</p>


	<h7> 
	<h3>
	<a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>III. New Convolutional Model</h3>

	<p> 
	<b><i>Motivation:</i></b> Realizing that perhaps the VGG16 architecture was too large for our purposes, we implemented the convolutional model described in "Age and Gender Classification using Convolutional Neural Networks" (Levi & Tassner), which achieved about 50% accuracy at classifying people into age categories, each spanning approximately 6 years.
	</p>
	
	<p> 
	<b><i>Implementation:</i></b> The model involved three blocks of layers, each of which contained a convolutional layer with a ReLu activation function, a pooling layer, and a normalization layer.  After these blocks the data was flattened, and the model ended with three fully connected layers, each separated from the others by a dropout layer.  In the final layer, a softmax function was used to obtain a probability distribution giving the probability of the person being in each of our 100 age classes. You can see the full model in Figure 4.
	</p>
	
	<p>
	<b><i>Result:</i></b> As we trained our model, the training accuracy increased steadily, but the validation accuracy and loss showed no movement - evidence, we believe, of overfitting.  To deal with this issue, we tried increasing the size of our dataset, but ran into memory/space errors.  We also tried adding extra dropout layers after each convolutional block, but these just prevented the network from training at all.  We also tried adjusting our batch size and learning rate, but these had no noticeable affect on training.
	</p>

	<h3>
	<a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>IV. Larger Age Classes</h3>

	<p> 
	<b><i>Motivation:</i></b> We realized that trying to classify faces into 100 age classes is difficult, as the difference between the features of people one year apart is negligible. We decided to instead use 5 larger age classes which (hopefully) would have more distinctive characteristics. This was also the approach used by Levi & Tassner in their paper  "Age and Gender Classification using Convolutional Neural Networks."
	</p>
	
	<p> 
	<b><i>Implementation:</i></b> We used 5 age classes, each of which covered a 20-year spread (0-20, 21-40, 41-60, 61-80, 81-100).
	</p>
	
	<p> 
	<b><i>Result:</i></b> The initial accuracy jumped up dramatically (as expected, because there were fewer classes).  Like before, during training, the accuracy increased and the loss decreased for our training set.  For the first 7-10 epochs, the loss and accuracy of the validation set fell/rose similarly to the training data, but quickly the validation loss and accuracy plateaued.
	</p>


	<h3>
	<a id="user-content-header-3" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>V. Balanced Data Set</h3>

	<p> 
	<b><i>Motivation:</i></b> Our data set is heavily skewed toward people age 20-40, which means that most of our samples belong in just one class.
	</p>
	
	<p> 
	<b><i>Implementation:</i></b> We chose equal numbers of samples from each age class. In total, we used 2000 images for training.
	</p>
	
	<p> 
	<b><i>Result:</i></b> There were no noticeable improvements. The result of one round of training is displayed in Figure 5.
	</p>

	<div>
		<img src="pics/trainingLoss.png" class="graph"/>
		<img src="pics/trainingAccuracy.png" class="graph"/>
		<br/>Figure 5: Loss and training accuracy over 41 epochs.<br/>
	</div>
	


	<hr>
	<h1> 
	<a id="user-content-header-1" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h1>
	<p> 
		Our network was relatively unsuccessful at estimating the age of individuals in the data set. We believe these are contributing factors:


		<ol>
			<Li>Overfitting: Even in our most functional models, the training set increased in accuracy until it plateaued near 100%, but the validation set quickly stopped increasing in accuracy. Although we tried to avoid this with methods such as incrasing the number of dropout layers and using more images, overfitting still remains a large problem.</Li>
			<Li>Difficult classification problem: Age estimation is a difficult problem, because people's apparent age may differ from their actual age. Our data set made the problem even more difficult. The images featured faces at a variety of angles, some with makeup or hats, making it more difficult to identify trends.</Li>
			<Li>Data set size: Out data set contained over 500k images, but were were only able to use a small subset of them due to hardware restrictions.  Training on the entire data set could help us improve accuracy. </Li>
			<Li>The original paper fine-tuned their model based on pretained weights which is the winner of previous age estimation challenge. However, this set of weights is only available for Caffe framework. It was late for us to shift from Keras to Caffe. </Li>
		</ol>

	</p>
	  
	  <p> But the result of our network is reasonable and improved after above attemps.</p>
		<div>
		<img src="pics/comparison.png" class="graph"/>
		<br/>Figure 6: Result from other implementation.<br/>	
	  	</div>
	      The above diagram is the result from another implementation that uses IMDB-WIKI dataset and convolutional network. They choose the approach
	      of finetuning on WideResNet instead of Caffe pretrained weights. As we could see, their network has the overfitting problem as well.
	      The training accuracy soars while validation accuracy barely changes. Combine on our result and theirs, we think the keys to success of this problem are
	      right weights to fine-tune on, and less biased datasets. 
	  <p> The progress we've made through the above 5 attemps, is our network does learn something though the improvement is not very significant
	      . During first seven epochs, validation loss keeps decreasing and validation accuracy increases from 20% to 30%, which approximately matches how fast training accuracy increases. </p>
	
		
	
	<hr>
	<h1> 
	<a id="user-content-header-1" class="anchor" href="#header-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Future Work</h1>
	  <p> 
		For future work, we would like to improve and extend the current network in following ways

	        <ol>    <Li>To improve our convolutional sets, a better dataset could give significantly improved results. Also, we received the advice during presentation that 40% margin might be one source of overfitting. Though this is 
			the strategy used in original paper to improve accuracy, we are curious whether extra context information contributes to overfitting, or better accuracy. </Li>
			<Li>Age and gender might be two correlated labels. So the problem is not only a multi-class but also a multi-label problem. We want to try out the approach of multi-label classification on this problem.</Li>
			<Li>Our original initial plan includes age estimation with self organizing map. Due to time issue, we didn't get the chance to do it. This topic is something we would like to explore try in the future. </Li>
			<Li>Use our model to solve more real world problems. For example, Nexar has coming data challenge of recognzing signal lights to improve self driving car technology. These real world problems which are solvable by 
				convolutional network interest us a lot. </Li>
	  	</ol>

	</p>

	<footer class="site-footer">
		<span class="site-footer-owner"> Our code repository is here </span>
	  <span class="site-footer-owner"><a href="https://github.com/jasonlong/cayman-theme">Cayman</a> is maintained by <a href="https://github.com/jasonlong">jasonlong</a>.</span>
	  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
	</footer>

    </section>

  </body>
</html>
